{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "{'accuracy': 0.7211934156378601, 'precision': 0.7311460255509488, 'recall': 0.7211934156378601, 'f1_score': 0.7182227828757093, 'auc': None}\n",
      "Test Metrics:\n",
      "{'accuracy': 0.7429305912596401, 'precision': 0.7461524389172034, 'recall': 0.7429305912596401, 'f1_score': 0.7385397523789776, 'auc': None}\n",
      "{'accuracy': 0.7429305912596401, 'precision': 0.7461524389172034, 'recall': 0.7429305912596401, 'f1_score': 0.7385397523789776, 'auc': None}\n"
     ]
    }
   ],
   "source": [
    "# Random forest model you provided that did not run\n",
    "\n",
    "import torch, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def load_dict_from_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        loaded_dict = pickle.load(file)\n",
    "    return loaded_dict\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    # Load data\n",
    "    X = torch.load(f'{file_path}/X.pt')\n",
    "    Y = torch.load(f'{file_path}/Y.pt')\n",
    "    \n",
    "    # # Check if X is a sparse matrix and convert to dense if necessary\n",
    "    # if isinstance(X, csr_matrix):\n",
    "    #     X = X.toarray()  # Convert sparse to dense\n",
    "\n",
    "    # Y = transform_y_for_multi_class(Y)\n",
    "    Nodes = load_dict_from_pickle(f'{file_path}/Nodes.pkl')\n",
    "\n",
    "    # # Subset to patient nodes (assuming node name starts with 'V')\n",
    "    # indeces = {p: i for i, p in enumerate(Nodes)}\n",
    "    # visit_indeces = [indeces[v] for v in Nodes if v[0] == 'V']\n",
    "\n",
    "    # X = X[visit_indeces]\n",
    "    # Y = Y[visit_indeces]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# Transform Y for Multi-Class Classification\n",
    "def transform_y_for_multi_class(Y):\n",
    "    # Convert LoS into 4 classes:\n",
    "    # 0 if LoS <= 1 (one day or less),\n",
    "    # 1 if LoS > 1 and LoS <= 3 (three days),\n",
    "    # 2 if LoS > 3 and LoS <= 7 (up to a week),\n",
    "    # 3 if LoS > 7 (more than a week)\n",
    "    return np.where(Y <= 1, 0,\n",
    "                    np.where(Y <= 3, 1,\n",
    "                             np.where(Y <= 7, 2, 3)))\n",
    "\n",
    "# 1. Apply SMOTE\n",
    "def apply_smote(X, Y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, Y_resampled = smote.fit_resample(X, Y)\n",
    "    return X_resampled, Y_resampled\n",
    "\n",
    "# 2. Train Random Forest\n",
    "def train_random_forest(X, Y, num_classes=4, num_estimators=100):\n",
    "    # Split data into training, validation, and test sets\n",
    "    X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, train_size=0.8, random_state=42)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, train_size=0.5, random_state=42)\n",
    "\n",
    "    # Apply SMOTE to the training set\n",
    "    X_train_resampled, Y_train_resampled = apply_smote(X_train, Y_train)\n",
    "\n",
    "    # Initialize the Random Forest Classifier\n",
    "    clf = RandomForestClassifier(n_estimators=num_estimators, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(X_train_resampled, Y_train_resampled)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    Y_val_pred = clf.predict(X_val)\n",
    "    val_metrics = evaluate_multi_class(Y_val, Y_val_pred, num_classes)\n",
    "\n",
    "    print(\"Validation Metrics:\")\n",
    "    print(val_metrics)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    Y_test_pred = clf.predict(X_test)\n",
    "    test_metrics = evaluate_multi_class(Y_test, Y_test_pred, num_classes)\n",
    "\n",
    "    print(\"Test Metrics:\")\n",
    "    print(test_metrics)\n",
    "\n",
    "    return clf, test_metrics\n",
    "\n",
    "# 3. Evaluation for Multi-Class Classification\n",
    "def evaluate_multi_class(true_labels, preds, num_classes):\n",
    "    # Binarize the true labels for multi-class AUC calculation\n",
    "    true_labels_binarized = label_binarize(true_labels, classes=np.arange(num_classes))\n",
    "\n",
    "    try:\n",
    "        # Calculate AUC for each class and then macro-average\n",
    "        auc = roc_auc_score(true_labels_binarized, label_binarize(preds, classes=np.arange(num_classes)), average='macro', multi_class='ovr')\n",
    "    except ValueError:\n",
    "        auc = None\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    precision = precision_score(true_labels, preds, average='weighted')\n",
    "    recall = recall_score(true_labels, preds, average='weighted')\n",
    "    f1 = f1_score(true_labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "# Main Function\n",
    "def main_random_forest(file_path, num_estimators=100):\n",
    "    # Load data (replace with actual data loading)\n",
    "    X, Y = load_data(file_path)\n",
    "\n",
    "    # Train Random Forest\n",
    "    clf, metrics = train_random_forest(X, Y, num_classes=4, num_estimators=num_estimators)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Set parameters and run the main training process\n",
    "saving_path = '../../Data/infectious'\n",
    "results = main_random_forest(saving_path, num_estimators=100)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X after loading: <class 'numpy.ndarray'>\n",
      "Type of Y after loading: <class 'numpy.ndarray'>\n",
      "Shape of X: (19442, 1456)\n",
      "Shape of Y: (19442,)\n",
      "Length of Nodes: 106822\n",
      "{0, 1, 2}\n",
      "{0, 1}\n",
      "Validation Metrics:\n",
      "{'accuracy': 0.808641975308642, 'precision': 0.8097579990601905, 'recall': 0.808641975308642, 'f1_score': 0.8090946457127548, 'auc': None}\n",
      "Test Metrics:\n",
      "{'accuracy': 0.8133676092544987, 'precision': 0.814702485330709, 'recall': 0.8133676092544987, 'f1_score': 0.8138759925547685, 'auc': None}\n",
      "Final Test Metrics:\n",
      "{'accuracy': 0.8133676092544987, 'precision': 0.814702485330709, 'recall': 0.8133676092544987, 'f1_score': 0.8138759925547685, 'auc': None}\n"
     ]
    }
   ],
   "source": [
    "# Put above into chat gpt and changed some of the code as node length did not match up with X.\n",
    "# Look at output \n",
    "\n",
    "# With def transform_y_for_multi_class(Y):\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def load_dict_from_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        loaded_dict = pickle.load(file)\n",
    "    return loaded_dict\n",
    "\n",
    "def load_data(file_path):\n",
    "    import scipy.sparse\n",
    "\n",
    "    # Load data\n",
    "    X = torch.load(f'{file_path}/X.pt')\n",
    "    Y = torch.load(f'{file_path}/Y.pt')\n",
    "    Nodes = load_dict_from_pickle(f'{file_path}/Nodes.pkl')\n",
    "\n",
    "    # Debugging: Print types and shapes\n",
    "    print(f\"Type of X after loading: {type(X)}\")\n",
    "    print(f\"Type of Y after loading: {type(Y)}\")\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of Y: {Y.shape}\")\n",
    "    print(f\"Length of Nodes: {len(Nodes)}\")\n",
    "\n",
    "    # Convert to NumPy arrays if necessary\n",
    "    if isinstance(X, torch.Tensor):\n",
    "        X = X.numpy()\n",
    "    if isinstance(Y, torch.Tensor):\n",
    "        Y = Y.numpy()\n",
    "\n",
    "    # Ensure X is a NumPy array\n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "\n",
    "    # Transform Y for classification\n",
    "    print(set(Y))\n",
    "    Y = transform_y_for_multi_class(Y)\n",
    "    print(set(Y))\n",
    "\n",
    "    # # Adjust Nodes to match X if necessary\n",
    "    # if len(Nodes) > X.shape[0]:\n",
    "    #     print(\"Warning: Nodes has more entries than X. Adjusting Nodes to match X.\")\n",
    "    #     Nodes = Nodes[:X.shape[0]]\n",
    "    #     print(len(Nodes))\n",
    "\n",
    "    # # Create indices mapping\n",
    "    # indices = {p: i for i, p in enumerate(Nodes)}\n",
    "    # visit_indices = [indices[v] for v in Nodes if v[0] == 'V']\n",
    "\n",
    "    # # Check for invalid indices\n",
    "    # invalid_indices = [idx for idx in visit_indices if idx >= X.shape[0]]\n",
    "    # if invalid_indices:\n",
    "    #     print(f\"Invalid indices found: {invalid_indices}\")\n",
    "    #     visit_indices = [idx for idx in visit_indices if idx < X.shape[0]]\n",
    "\n",
    "    # # Subset X and Y\n",
    "    # X = X[visit_indices]\n",
    "    # Y = Y[visit_indices]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# Rest of your code remains the same...\n",
    "\n",
    "def transform_y_for_multi_class(Y):\n",
    "    # Convert LoS into 4 classes:\n",
    "    # 0 if LoS <= 1 (one day or less),\n",
    "    # 1 if LoS > 1 and LoS <= 3 (three days),\n",
    "    # 2 if LoS > 3 and LoS <= 7 (up to a week),\n",
    "    # 3 if LoS > 7 (more than a week)\n",
    "    return np.where(Y <= 1, 0,\n",
    "                    np.where(Y <= 3, 1,\n",
    "                             np.where(Y <= 7, 2, 3)))\n",
    "\n",
    "def apply_smote(X, Y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, Y_resampled = smote.fit_resample(X, Y)\n",
    "    \n",
    "    # Ensure data is in the correct format\n",
    "    if scipy.sparse.issparse(X_resampled):\n",
    "        X_resampled.indptr = X_resampled.indptr.astype(np.int32)\n",
    "        X_resampled.indices = X_resampled.indices.astype(np.int32)\n",
    "        X_resampled = X_resampled.toarray()\n",
    "    else:\n",
    "        # Ensure X_resampled is a NumPy array\n",
    "        X_resampled = np.asarray(X_resampled)\n",
    "    \n",
    "    return X_resampled, Y_resampled\n",
    "\n",
    "def train_random_forest(X, Y, num_classes=4, num_estimators=100):\n",
    "    # Split data into training, validation, and test sets\n",
    "    X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, train_size=0.8, random_state=42)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, train_size=0.5, random_state=42)\n",
    "\n",
    "    # Apply SMOTE to the training set\n",
    "    X_train_resampled, Y_train_resampled = apply_smote(X_train, Y_train)\n",
    "\n",
    "    # Initialize the Random Forest Classifier\n",
    "    clf = RandomForestClassifier(n_estimators=num_estimators, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(X_train_resampled, Y_train_resampled)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    Y_val_pred = clf.predict(X_val)\n",
    "    val_metrics = evaluate_multi_class(Y_val, Y_val_pred, num_classes)\n",
    "\n",
    "    print(\"Validation Metrics:\")\n",
    "    print(val_metrics)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    Y_test_pred = clf.predict(X_test)\n",
    "    test_metrics = evaluate_multi_class(Y_test, Y_test_pred, num_classes)\n",
    "\n",
    "    print(\"Test Metrics:\")\n",
    "    print(test_metrics)\n",
    "\n",
    "    return clf, test_metrics\n",
    "\n",
    "def evaluate_multi_class(true_labels, preds, num_classes):\n",
    "    # Binarize the labels for multi-class AUC calculation\n",
    "    true_labels_binarized = label_binarize(true_labels, classes=np.arange(num_classes))\n",
    "    preds_binarized = label_binarize(preds, classes=np.arange(num_classes))\n",
    "\n",
    "    try:\n",
    "        # Calculate AUC for each class and then macro-average\n",
    "        auc = roc_auc_score(true_labels_binarized, preds_binarized, average='macro', multi_class='ovr')\n",
    "    except ValueError:\n",
    "        auc = None\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    precision = precision_score(true_labels, preds, average='weighted')\n",
    "    recall = recall_score(true_labels, preds, average='weighted')\n",
    "    f1 = f1_score(true_labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "def main_random_forest(file_path, num_estimators=100):\n",
    "    # Load data\n",
    "    X, Y = load_data(file_path)\n",
    "\n",
    "    # Train Random Forest\n",
    "    clf, metrics = train_random_forest(X, Y, num_classes=4, num_estimators=num_estimators)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Set parameters and run the main training process\n",
    "if __name__ == \"__main__\":\n",
    "    saving_path = '../../Data/infectious'  # Adjust the path as needed\n",
    "    results = main_random_forest(saving_path, num_estimators=100)\n",
    "    print(\"Final Test Metrics:\")\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X after loading: <class 'numpy.ndarray'>\n",
      "Type of Y after loading: <class 'numpy.ndarray'>\n",
      "Shape of X: (19442, 1456)\n",
      "Shape of Y: (19442,)\n",
      "Length of Nodes: 106822\n",
      "Class distribution in Y_train: [1962 4267 9324]\n",
      "Class distribution in Y_val: [ 237  529 1178]\n",
      "Class distribution in Y_test: [ 193  582 1170]\n",
      "Validation Metrics:\n",
      "{'accuracy': 0.7211934156378601, 'precision': 0.7311460255509488, 'recall': 0.7211934156378601, 'f1_score': 0.7182227828757093, 'auc': 0.7215652478846941}\n",
      "Test Metrics:\n",
      "{'accuracy': 0.7429305912596401, 'precision': 0.7461524389172034, 'recall': 0.7429305912596401, 'f1_score': 0.7385397523789776, 'auc': 0.733720940579991}\n",
      "Final Test Metrics:\n",
      "{'accuracy': 0.7429305912596401, 'precision': 0.7461524389172034, 'recall': 0.7429305912596401, 'f1_score': 0.7385397523789776, 'auc': 0.733720940579991}\n"
     ]
    }
   ],
   "source": [
    "# Without def transform_y_for_multi_class(Y):\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def load_dict_from_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        loaded_dict = pickle.load(file)\n",
    "    return loaded_dict\n",
    "\n",
    "def load_data(file_path):\n",
    "    # Load data\n",
    "    X = torch.load(f'{file_path}/X.pt')\n",
    "    Y = torch.load(f'{file_path}/Y.pt')\n",
    "    Nodes = load_dict_from_pickle(f'{file_path}/Nodes.pkl')\n",
    "\n",
    "    # Debugging: Print types and shapes\n",
    "    print(f\"Type of X after loading: {type(X)}\")\n",
    "    print(f\"Type of Y after loading: {type(Y)}\")\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of Y: {Y.shape}\")\n",
    "    print(f\"Length of Nodes: {len(Nodes)}\")\n",
    "\n",
    "    # Convert to NumPy arrays if necessary\n",
    "    if isinstance(X, torch.Tensor):\n",
    "        X = X.numpy()\n",
    "    if isinstance(Y, torch.Tensor):\n",
    "        Y = Y.numpy()\n",
    "\n",
    "    # Ensure X is a NumPy array\n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "\n",
    "    # No need to transform Y, it already has 3 classes: [0, 1, 2]\n",
    "\n",
    "    # # Adjust Nodes to match X if necessary\n",
    "    # if len(Nodes) > X.shape[0]:\n",
    "    #     print(\"Warning: Nodes has more entries than X. Adjusting Nodes to match X.\")\n",
    "    #     Nodes = Nodes[:X.shape[0]]\n",
    "    #     print(len(Nodes))\n",
    "\n",
    "    # # Create indices mapping\n",
    "    # indices = {p: i for i, p in enumerate(Nodes)}\n",
    "    # visit_indices = [indices[v] for v in Nodes if v[0] == 'V']\n",
    "\n",
    "    # # Check for invalid indices\n",
    "    # invalid_indices = [idx for idx in visit_indices if idx >= X.shape[0]]\n",
    "    # if invalid_indices:\n",
    "    #     print(f\"Invalid indices found: {invalid_indices}\")\n",
    "    #     visit_indices = [idx for idx in visit_indices if idx < X.shape[0]]\n",
    "\n",
    "    # # Subset X and Y\n",
    "    # X = X[visit_indices]\n",
    "    # Y = Y[visit_indices]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def apply_smote(X, Y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, Y_resampled = smote.fit_resample(X, Y)\n",
    "\n",
    "    # Ensure data is in the correct format\n",
    "    X_resampled = np.asarray(X_resampled)\n",
    "    \n",
    "    return X_resampled, Y_resampled\n",
    "\n",
    "def train_random_forest(X, Y, num_classes=3, num_estimators=100):\n",
    "    # Split data into training, validation, and test sets\n",
    "    X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, train_size=0.8, random_state=42)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, train_size=0.5, random_state=42)\n",
    "\n",
    "    # Debugging: Check class distributions after splitting\n",
    "    print(f\"Class distribution in Y_train: {np.bincount(Y_train)}\")\n",
    "    print(f\"Class distribution in Y_val: {np.bincount(Y_val)}\")\n",
    "    print(f\"Class distribution in Y_test: {np.bincount(Y_test)}\")\n",
    "\n",
    "    # Apply SMOTE to the training set only if there are multiple classes\n",
    "    if len(np.unique(Y_train)) > 1:\n",
    "        X_train_resampled, Y_train_resampled = apply_smote(X_train, Y_train)\n",
    "    else:\n",
    "        print(\"Skipping SMOTE as there's only one class in Y_train.\")\n",
    "        X_train_resampled, Y_train_resampled = X_train, Y_train\n",
    "\n",
    "    # Initialize the Random Forest Classifier\n",
    "    clf = RandomForestClassifier(n_estimators=num_estimators, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(X_train_resampled, Y_train_resampled)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    Y_val_pred = clf.predict(X_val)\n",
    "    val_metrics = evaluate_multi_class(Y_val, Y_val_pred, num_classes)\n",
    "\n",
    "    print(\"Validation Metrics:\")\n",
    "    print(val_metrics)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    Y_test_pred = clf.predict(X_test)\n",
    "    test_metrics = evaluate_multi_class(Y_test, Y_test_pred, num_classes)\n",
    "\n",
    "    print(\"Test Metrics:\")\n",
    "    print(test_metrics)\n",
    "\n",
    "    return clf, test_metrics\n",
    "\n",
    "def evaluate_multi_class(true_labels, preds, num_classes=3):\n",
    "    # Binarize the labels for multi-class AUC calculation\n",
    "    true_labels_binarized = label_binarize(true_labels, classes=np.arange(num_classes))\n",
    "    preds_binarized = label_binarize(preds, classes=np.arange(num_classes))\n",
    "\n",
    "    try:\n",
    "        # Calculate AUC for each class and then macro-average\n",
    "        auc = roc_auc_score(true_labels_binarized, preds_binarized, average='macro', multi_class='ovr')\n",
    "    except ValueError:\n",
    "        auc = None\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    precision = precision_score(true_labels, preds, average='weighted')\n",
    "    recall = recall_score(true_labels, preds, average='weighted')\n",
    "    f1 = f1_score(true_labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "def main_random_forest(file_path, num_estimators=100):\n",
    "    # Load data\n",
    "    X, Y = load_data(file_path)\n",
    "\n",
    "    # Train Random Forest\n",
    "    clf, metrics = train_random_forest(X, Y, num_classes=3, num_estimators=num_estimators)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Set parameters and run the main training process\n",
    "if __name__ == \"__main__\":\n",
    "    saving_path = '../../Data/infectious'  # Adjust the path as needed\n",
    "    results = main_random_forest(saving_path, num_estimators=100)\n",
    "    print(\"Final Test Metrics:\")\n",
    "    print(results)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
