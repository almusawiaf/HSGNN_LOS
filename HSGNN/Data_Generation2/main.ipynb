{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the data\n",
    "1. Generate the Heterogeneous graph\n",
    "2. Generate the feature set from the clinical notes.\n",
    "3. Generate the Labels\n",
    "4. Generate the k-metapath-based similarity matrices\n",
    "5. Convert the As to edge-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class heterogeneous_Graph:\n",
    "    def __init__(self, G):\n",
    "        self.HG = G\n",
    "        Nodes = list(self.HG.nodes())\n",
    "        self.Patients =    [v for v in Nodes if v[0]=='C']\n",
    "        self.Visits =      [v for v in Nodes if v[0]=='V']\n",
    "        self.Medications = [v for v in Nodes if v[0]=='M']\n",
    "        self.Diagnoses  =  [v for v in Nodes if v[0]=='D']\n",
    "        self.Procedures =  [v for v in Nodes if v[0]=='P']\n",
    "        self.Labs       =  [v for v in Nodes if v[0]=='L']\n",
    "        self.MicroBio   =  [v for v in Nodes if v[0]=='B']\n",
    "        self.Nodes = self.Patients  + self.Visits + self.Medications + self.Diagnoses + self.Procedures + self.Labs + self.MicroBio\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 DMPLB2 PC 250 True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "\n",
    "def save_list_as_pickle(L, given_path, file_name):\n",
    "    import pickle\n",
    "    print(f'saving to {given_path}/{file_name}.pkl')\n",
    "    with open(f'{given_path}/{file_name}.pkl', 'wb') as file:\n",
    "        pickle.dump(L, file)\n",
    "\n",
    "\n",
    "num_Diseases    = int(os.getenv('NUM_DISEASES', 203))  \n",
    "DISEASE_FILE    = os.getenv('DISEASE_FILE', f'DMPLB2')  \n",
    "similarity_type = os.getenv('similarity_type', 'PC')  # options are PC: PathCount, SPS: Symmetric PathSim\n",
    "\n",
    "num_Sample      = int(os.getenv('num_Sample', 250))  \n",
    "r_u_sampling    = os.getenv('r_u_sampling', 'True')  \n",
    "SNF_ing         = os.getenv('SNF_ing', 'True')  \n",
    "\n",
    "\n",
    "if r_u_sampling=='True':\n",
    "    sampling = True\n",
    "else:\n",
    "    sampling = False\n",
    "\n",
    "if SNF_ing=='True':\n",
    "    SNF_ing = True\n",
    "else:\n",
    "    SNF_ing = False\n",
    "\n",
    "disease_data_path = '/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data'\n",
    "\n",
    "print(num_Diseases, DISEASE_FILE, similarity_type, num_Sample, sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "saving_path = f'{disease_data_path}/{num_Diseases}_Diagnoses/{DISEASE_FILE}/{num_Sample}'\n",
    "\n",
    "for p in ['HGNN_data', 'clinical_items', 'GMLs', 'OHV', 'PSGs', 'SNFs']:\n",
    "    os.makedirs(f'{saving_path}/{p}', exist_ok=True)\n",
    "\n",
    "saving_path = f'{disease_data_path}/{num_Diseases}_Diagnoses/{DISEASE_FILE}/{num_Sample}/HGNN_data'\n",
    "os.makedirs(f'{saving_path}/As', exist_ok=True)\n",
    "# =================================================================================\n",
    "\n",
    "complete_HG = nx.read_gml(f'{disease_data_path}/{num_Diseases}_Diagnoses/complete_HG.gml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating complete HG from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module1 import generating_HG as gHG\n",
    "# HG_inst = gHG.Generate_HG()\n",
    "# nx.write_gml(HG_inst.HG, f'{disease_data_path}/{num_Diseases}_Diagnoses/complete_HG.gml')\n",
    "# gHG.G_statistics(HG_inst.HG)\n",
    "# # ======================To sample or not to sample, that is the question =========================\n",
    "# if not sampling:\n",
    "#     num_Sample = len(HG_inst.Patients)\n",
    "#     HG = HG_inst.HG\n",
    "# else:\n",
    "#     patients_to_remove = random.sample(HG_inst.Patients, len(HG_inst.Patients) - num_Sample)\n",
    "#     print(len(patients_to_remove), num_Sample, len(HG_inst.Patients))\n",
    "    \n",
    "#     # deleting the nodes\n",
    "#     HG = gHG.remove_patients_and_linked_visits(patients_to_remove, HG_inst.HG)\n",
    "# # ================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole graph or Sample graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of patients = 46437\n",
      "number of visits = 58929\n",
      "number of Medication = 592\n",
      "number of Diagnoses = 203\n",
      "number of Procedures = 89\n",
      "number of Labs = 480\n",
      "number of MicoBio = 64\n",
      "number of Edges = 5336561\n",
      "------------------------------------------\n",
      "\n",
      "46187 250 46437\n",
      "Number of PATIENTS to remove: 46187\n",
      "Number of nodes to remove: 104640\n"
     ]
    }
   ],
   "source": [
    "HG_obj = heterogeneous_Graph(complete_HG)\n",
    "\n",
    "gHG.G_statistics(HG_obj.HG)\n",
    "\n",
    "# ======================To sample or not to sample, that is the question =========================\n",
    "if not sampling:\n",
    "    num_Sample = len(HG_obj.Patients)\n",
    "    HG = HG_obj.HG\n",
    "else:\n",
    "    patients_to_remove = random.sample(HG_obj.Patients, len(HG_obj.Patients) - num_Sample)\n",
    "    print(len(patients_to_remove), num_Sample, len(HG_obj.Patients))\n",
    "    \n",
    "    # deleting the nodes\n",
    "    HG = gHG.remove_patients_and_linked_visits(patients_to_remove, HG_obj.HG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting the feature set for all nodes\n",
      "getting the feature set for all nodes: visit_level\n",
      "getting the feature set for all nodes\n",
      "getting the feature set for all nodes: visit_level\n"
     ]
    }
   ],
   "source": [
    "from module1 import XY_preparation as XY\n",
    "# ============================ Extracting Patient-based X and Y =================================\n",
    "XY_inst = XY.XY_preparation(HG)\n",
    "X = XY_inst.X\n",
    "Y = XY_inst.Y\n",
    "# ============================ Extracting Visit-based X and Y =================================\n",
    "XY_inst = XY.XY_preparation(HG)\n",
    "XV = XY_inst.X_visit\n",
    "YV = XY_inst.Y_visit\n",
    "# ==================================== Saving X and Y  (patient-based) ============================\n",
    "torch.save(X, f'{saving_path}/X.pt')\n",
    "torch.save(Y, f'{saving_path}/Y.pt')\n",
    "# ==================================== Saving X and Y (visit-based) =================================\n",
    "torch.save(X, f'{saving_path}/XV.pt')\n",
    "torch.save(Y, f'{saving_path}/YV.pt')\n",
    "del X\n",
    "del Y\n",
    "del XV\n",
    "del YV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-path Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting As from HG\n",
      "\n",
      "=============================================================\n",
      "Multiplication phase...\n",
      "\n",
      "Matrix multiplication on GPU took 3.148261 seconds\n",
      "Matrix multiplication on GPU took 0.009920 seconds\n",
      "Matrix multiplication on GPU took 0.003654 seconds\n",
      "Matrix multiplication on GPU took 0.003648 seconds\n",
      "Matrix multiplication on GPU took 0.124902 seconds\n",
      "Matrix multiplication on GPU took 0.091581 seconds\n",
      "Matrix multiplication on GPU took 0.076202 seconds\n",
      "Matrix multiplication on GPU took 0.009122 seconds\n",
      "Matrix multiplication on GPU took 0.003656 seconds\n",
      "Matrix multiplication on GPU took 0.003622 seconds\n",
      "Matrix multiplication on GPU took 0.005882 seconds\n",
      "Matrix multiplication on GPU took 0.004503 seconds\n",
      "Matrix multiplication on GPU took 0.053354 seconds\n",
      "Matrix multiplication on GPU took 0.023352 seconds\n",
      "Matrix multiplication on GPU took 0.010165 seconds\n",
      "Matrix multiplication on GPU took 0.022702 seconds\n",
      "Matrix multiplication on GPU took 0.005730 seconds\n",
      "Matrix multiplication on GPU took 0.008958 seconds\n",
      "Matrix multiplication on GPU took 0.003592 seconds\n",
      "Matrix multiplication on GPU took 0.009753 seconds\n",
      "Matrix multiplication on GPU took 0.009724 seconds\n",
      "Matrix multiplication on GPU took 0.004437 seconds\n",
      "Matrix multiplication on GPU took 0.018539 seconds\n",
      "Matrix multiplication on GPU took 0.015219 seconds\n",
      "Matrix multiplication on GPU took 0.017868 seconds\n",
      "Matrix multiplication on GPU took 0.029256 seconds\n",
      "Matrix multiplication on GPU took 0.008209 seconds\n",
      "Matrix multiplication on GPU took 0.007130 seconds\n",
      "Matrix multiplication on GPU took 0.008116 seconds\n",
      "Matrix multiplication on GPU took 0.004193 seconds\n",
      "Matrix multiplication on GPU took 0.028218 seconds\n",
      "Matrix multiplication on GPU took 0.008694 seconds\n",
      "Matrix multiplication on GPU took 0.045564 seconds\n",
      "Matrix multiplication on GPU took 0.049131 seconds\n",
      "Matrix multiplication on GPU took 0.007574 seconds\n",
      "Matrix multiplication on GPU took 0.012558 seconds\n",
      "Matrix multiplication on GPU took 0.028893 seconds\n",
      "Matrix multiplication on GPU took 0.005037 seconds\n",
      "Matrix multiplication on GPU took 0.008780 seconds\n",
      "Matrix multiplication on GPU took 0.010693 seconds\n",
      "Matrix multiplication on GPU took 0.003603 seconds\n",
      "Matrix multiplication on GPU took 0.003603 seconds\n",
      "Matrix multiplication on GPU took 0.009653 seconds\n",
      "Matrix multiplication on GPU took 0.016634 seconds\n",
      "Matrix multiplication on GPU took 0.022516 seconds\n",
      "=============================================================\n",
      "Multiplication phase completed!\n",
      "\n",
      "saving to /lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data/203_Diagnoses/DMPLB2/250/HGNN_data/As/selected_i.pkl\n",
      "saving to /lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data/203_Diagnoses/DMPLB2/250/HGNN_data/Nodes.pkl\n"
     ]
    }
   ],
   "source": [
    "from module1 import meta_path_GPU as MP\n",
    "# ======================= Computing the Meta Path based Similarities ======================\n",
    "MP_inst = MP.Meta_path(HG, similarity_type = 'PC', saving_path = saving_path)\n",
    "# MP_inst = Meta_path(HG, similarity_type = 'PC', saving_path = saving_path)\n",
    "# ==================================== SAVING =============================================\n",
    "nx.write_gml(HG, f'{saving_path}/HG.gml')\n",
    "save_list_as_pickle(MP_inst.Nodes,   saving_path, 'Nodes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data/203_Diagnoses/DMPLB2/250/HGNN_data/As/sparse_matrix_0.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data_Generation2/main.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data_Generation2/main.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodule1\u001b[39;00m \u001b[39mimport\u001b[39;00m reduction \u001b[39mas\u001b[39;00m Red\n\u001b[0;32m----> <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data_Generation2/main.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m reduction_obj \u001b[39m=\u001b[39m Red\u001b[39m.\u001b[39;49mReduction(saving_path)\n",
      "File \u001b[0;32m~/PhD_Projects/HGNN_Project2/Data_Generation2/module1/reduction.py:36\u001b[0m, in \u001b[0;36mReduction.__init__\u001b[0;34m(self, saving_path, gpu)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, saving_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, gpu \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 36\u001b[0m     Ws1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_Ws(saving_path, \u001b[39m'\u001b[39;49m\u001b[39mAs\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     37\u001b[0m     \u001b[39m# self.Ws2 = self.read_Ws(saving_path, 'Cosine_As')\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     Ws \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselecting_high_edges(Ws1)\n",
      "File \u001b[0;32m~/PhD_Projects/HGNN_Project2/Data_Generation2/module1/reduction.py:43\u001b[0m, in \u001b[0;36mReduction.read_Ws\u001b[0;34m(self, saving_path, folder_name)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_Ws\u001b[39m(\u001b[39mself\u001b[39m, saving_path, folder_name):\n\u001b[1;32m     42\u001b[0m     selected_i \u001b[39m=\u001b[39m load_dict_from_pickle(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msaving_path\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mfolder_name\u001b[39m}\u001b[39;00m\u001b[39m/selected_i.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m [get_edges_dict(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00msaving_path\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mfolder_name\u001b[39m}\u001b[39;49;00m\u001b[39m/sparse_matrix_\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m.npz\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m selected_i]\n",
      "File \u001b[0;32m~/PhD_Projects/HGNN_Project2/Data_Generation2/module1/reduction.py:43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_Ws\u001b[39m(\u001b[39mself\u001b[39m, saving_path, folder_name):\n\u001b[1;32m     42\u001b[0m     selected_i \u001b[39m=\u001b[39m load_dict_from_pickle(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msaving_path\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mfolder_name\u001b[39m}\u001b[39;00m\u001b[39m/selected_i.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m [get_edges_dict(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00msaving_path\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mfolder_name\u001b[39m}\u001b[39;49;00m\u001b[39m/sparse_matrix_\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m.npz\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m selected_i]\n",
      "File \u001b[0;32m~/PhD_Projects/HGNN_Project2/Data_Generation2/module1/reduction.py:146\u001b[0m, in \u001b[0;36mget_edges_dict\u001b[0;34m(the_path)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_edges_dict\u001b[39m(the_path):\n\u001b[0;32m--> 146\u001b[0m     A \u001b[39m=\u001b[39m sparse\u001b[39m.\u001b[39;49mload_npz(the_path)\n\u001b[1;32m    147\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(A, scipy\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39mcoo_matrix):\n\u001b[1;32m    148\u001b[0m         A \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mtocoo()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/scipy/sparse/_matrix_io.py:134\u001b[0m, in \u001b[0;36mload_npz\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_npz\u001b[39m(file):\n\u001b[1;32m     81\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Load a sparse array/matrix from a file using ``.npz`` format.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m    >>> sparse_array = sp.sparse.csr_array(tmp)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39;49mload(file, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mPICKLE_KWARGS) \u001b[39mas\u001b[39;00m loaded:\n\u001b[1;32m    135\u001b[0m         sparse_format \u001b[39m=\u001b[39m loaded\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    136\u001b[0m         \u001b[39mif\u001b[39;00m sparse_format \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/envCUDA/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39m(os_fspath(file), \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data/203_Diagnoses/DMPLB2/250/HGNN_data/As/sparse_matrix_0.npz'"
     ]
    }
   ],
   "source": [
    "from module1 import reduction as Red\n",
    "reduction_obj = Red.Reduction(saving_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
