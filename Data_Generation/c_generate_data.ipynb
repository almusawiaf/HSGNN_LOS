{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the data\n",
    "<!-- 1. Generate the Heterogeneous graph -->\n",
    "<!-- 2. Generate the feature set from the clinical notes. -->\n",
    "<!-- 3. Generate the Labels -->\n",
    "1. read the HG\n",
    "2. Generate the k-metapath-based similarity matrices\n",
    "3. Convert the As to edge-based.\n",
    "<!-- 6. Downsize the X to a vector of 32 values. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease File: all_MIMIC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_770446/1435998400.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X = torch.load(f'{HG_path}/X.pt')\n",
      "/tmp/ipykernel_770446/1435998400.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Y = torch.load(f'{HG_path}/Y.pt')\n"
     ]
    }
   ],
   "source": [
    "import sys, os, copy\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "sys.path.append('../data_generation')\n",
    "\n",
    "from  ModelFunctions import *\n",
    "# ============================================================================\n",
    "DISEASE_FILE = 'all_MIMIC'\n",
    "print(f\"Disease File: {DISEASE_FILE}\")\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "## Athena\n",
    "\n",
    "# OAK\n",
    "HG_path = f'../Data/HG'\n",
    "\n",
    "# reading the Nodes\n",
    "\n",
    "with open(f'{HG_path}/Nodes.pkl', 'rb') as file:\n",
    "    Nodes = pickle.load(file)\n",
    "\n",
    "X = torch.load(f'{HG_path}/X.pt')\n",
    "Y = torch.load(f'{HG_path}/Y.pt')\n",
    "\n",
    "HG = nx.read_gml(f'{HG_path}/HG.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_list_as_pickle(L, given_path, file_name):\n",
    "    # Check if the directory exists, if not, create it\n",
    "    if not os.path.exists(given_path):\n",
    "        os.makedirs(given_path)\n",
    "        print(f'Directory {given_path} created.')\n",
    "    \n",
    "    # Save the list as a pickle file\n",
    "    print(f'Saving to {given_path}/{file_name}.pkl')\n",
    "    with open(f'{given_path}/{file_name}.pkl', 'wb') as file:\n",
    "        pickle.dump(L, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_statistics(G):\n",
    "    Nodes = list(G.nodes())\n",
    "\n",
    "    Patients =    [v for v in Nodes if v[0]=='C']\n",
    "    Visits =      [v for v in Nodes if v[0]=='V']\n",
    "    Medications = [v for v in Nodes if v[0]=='M']\n",
    "    Diagnosis =   [v for v in Nodes if v[0]=='D']\n",
    "    Procedures =  [v for v in Nodes if v[0]=='P']\n",
    "\n",
    "    print(f'number of patients = {len(Patients)}')\n",
    "    print(f'number of visits = {len(Visits)}')\n",
    "    print(f'number of Medication = {len(Medications)}')\n",
    "    print(f'number of Diagnoses = {len(Diagnosis)}')\n",
    "    print(f'number of Procedures = {len(Procedures)}')\n",
    "    print(f'number of Edges = {G.number_of_edges()}')\n",
    "    \n",
    "    print('------------------------------------------\\n')\n",
    "\n",
    "def calculate_percentage_of_zeros(A):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of zero values in a 2D numpy array.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy.ndarray): 2D numpy array\n",
    "\n",
    "    Returns:\n",
    "    float: Percentage of zero values\n",
    "    \"\"\"\n",
    "    # Count the number of zero values\n",
    "    num_zeros = np.count_nonzero(A == 0)\n",
    "\n",
    "    # Count the total number of values\n",
    "    total_values = A.size\n",
    "\n",
    "    # Calculate the percentage of zero values\n",
    "    percentage_zeros = (num_zeros / total_values) * 100\n",
    "\n",
    "    return percentage_zeros\n",
    "\n",
    "# Removing <patients to delete> from HG\n",
    "def remove_patients_and_linked_visits(nodes, HG):\n",
    "    '''remove patients and their visits from HG'''\n",
    "    print('Number of PATIENTS to remove: ', len(nodes))\n",
    "    \n",
    "    new_HG = deepcopy(HG)\n",
    "    nodes_to_remove = []\n",
    "    for node in nodes:\n",
    "        for v in HG.neighbors(node):\n",
    "            if v[0]=='V':\n",
    "                nodes_to_remove.append(v)\n",
    "        nodes_to_remove.append(node)\n",
    "    print('Number of nodes to remove: ', len(nodes_to_remove))\n",
    "    new_HG.remove_nodes_from(nodes_to_remove)\n",
    "    return new_HG    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample of patients....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of patients = 46437\n",
      "number of visits = 58897\n",
      "number of Medication = 592\n",
      "number of Diagnoses = 203\n",
      "number of Procedures = 89\n",
      "number of patients = 46437\n",
      "number of visits = 58897\n",
      "number of Medication = 592\n",
      "number of Diagnoses = 203\n",
      "number of Procedures = 89\n",
      "number of Edges = 769929\n",
      "------------------------------------------\n",
      "\n",
      "Number of PATIENTS to remove:  34862\n",
      "Number of nodes to remove:  74349\n",
      "number of patients = 11575\n",
      "number of visits = 19410\n",
      "number of Medication = 592\n",
      "number of Diagnoses = 203\n",
      "number of Procedures = 89\n",
      "number of Edges = 325301\n",
      "------------------------------------------\n",
      "\n",
      "number of patients = 11575\n",
      "number of visits = 19410\n",
      "number of Medication = 592\n",
      "number of Diagnoses = 203\n",
      "number of Procedures = 89\n"
     ]
    }
   ],
   "source": [
    "disease_name = 'infectious'\n",
    "with open(f'{HG_path}/Patients_{disease_name}.pkl', 'rb') as file:\n",
    "    Sample_Patients = pickle.load(file)\n",
    "\n",
    "# ==============================================================\n",
    "# Updating the HG by removing other patients\n",
    "Patients, Visits, Medications, Diagnosis, Procedures, _ = get_Nodes(HG)\n",
    "Nodes = Patients + Visits + Medications + Diagnosis  + Procedures\n",
    "\n",
    "old_indeces = {p: i for i, p in enumerate(Nodes)}\n",
    "\n",
    "# Removing patients and linked information...\n",
    "nodes_to_delete = [n for n in Patients if n not in Sample_Patients]\n",
    "G_statistics(HG)    \n",
    "new_HG = remove_patients_and_linked_visits(nodes_to_delete, HG)\n",
    "G_statistics(new_HG)    \n",
    "del HG\n",
    "HG = deepcopy(new_HG)\n",
    "del new_HG\n",
    "\n",
    "# =======================================================================\n",
    "Patients, Visits, Medications, Diagnosis, Procedures, _ = get_Nodes(HG)\n",
    "Nodes = Patients + Visits + Medications + Diagnosis  + Procedures\n",
    "\n",
    "new_indeces = [old_indeces[v] for v in Nodes]\n",
    "\n",
    "NewX = X[new_indeces]\n",
    "NewY = Y[new_indeces]\n",
    "\n",
    "# ============================================================================\n",
    "saving_path = f'../Data/203_Diagnoses/{disease_name}'\n",
    "\n",
    "torch.save(NewX, f'{saving_path}/X.pt')\n",
    "torch.save(NewY, f'{saving_path}/Y.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visits Splitting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of patients = 11575\n",
      "number of visits = 19410\n",
      "number of Medication = 592\n",
      "number of Diagnoses = 203\n",
      "number of Procedures = 89\n",
      "Saving to ../Data/203_Diagnoses/infectious/V_train.pkl\n",
      "Saving to ../Data/203_Diagnoses/infectious/V_test.pkl\n",
      "Saving to ../Data/203_Diagnoses/infectious/V_val.pkl\n",
      "Saving to ../Data/203_Diagnoses/infectious/Nodes.pkl\n"
     ]
    }
   ],
   "source": [
    "Patients, Visits, Medications, Diagnosis, Procedures, _ = get_Nodes(HG)\n",
    "Nodes2 = Patients + Visits + Medications + Diagnosis  + Procedures\n",
    "# # =============================================================================\n",
    "# # =============================================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "P_train, P_temp = train_test_split(Visits, test_size=0.8, random_state=42)\n",
    "P_val, P_test = train_test_split(P_temp, test_size=1/2, random_state=42)\n",
    "\n",
    "len(P_train)/len(Patients), len(P_test)/len(Patients), len(P_val)/len(Patients)\n",
    "\n",
    "# ============================================================================\n",
    "save_list_as_pickle(P_train, saving_path, 'V_train')\n",
    "save_list_as_pickle(P_test,  saving_path, 'V_test')\n",
    "save_list_as_pickle(P_val,   saving_path, 'V_val')\n",
    "save_list_as_pickle(Nodes2,  saving_path, 'Nodes')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_adjacency_matrix(G, all_nodes, subset_nodes):\n",
    "    \"\"\"\n",
    "    Returns the adjacency matrix of a subset of nodes in the graph G,\n",
    "    keeping all other nodes in the matrix but without edges.\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The input graph.\n",
    "    subset_nodes (list): The list of nodes to keep edges for.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The modified adjacency matrix.\n",
    "    \"\"\"\n",
    "    # Get the full adjacency matrix\n",
    "    adj_matrix = nx.to_numpy_array(G)\n",
    "    \n",
    "    # Create a boolean mask to identify nodes not in the subset\n",
    "    mask = np.isin(all_nodes, subset_nodes)\n",
    "    \n",
    "    # For nodes not in the subset, set rows and columns in the adjacency matrix to zero\n",
    "    for i in range(len(all_nodes)):\n",
    "        if not mask[i]:\n",
    "            adj_matrix[i, :] = 0  # Zero out the row\n",
    "            adj_matrix[:, i] = 0  # Zero out the column\n",
    "    \n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(31869, 31869), (31869, 31869), (31869, 31869), (31869, 31869)]\n"
     ]
    }
   ],
   "source": [
    "# P_train, Visits, Medications, Diagnosis, Procedures, _ = get_Nodes(HG)\n",
    "# Nodes = P_train + Visits + Medications + Diagnosis  + Procedures\n",
    "\n",
    "# W_cv = subset_adjacency_matrix(HG, Nodes, P_train + Visits)\n",
    "W_cv = subset_adjacency_matrix(HG, Nodes2, Patients + Visits)\n",
    "W_vm = subset_adjacency_matrix(HG, Nodes2, Visits + Medications)\n",
    "W_vd = subset_adjacency_matrix(HG, Nodes2, Visits + Diagnosis)\n",
    "W_vp = subset_adjacency_matrix(HG, Nodes2, Visits + Procedures)  \n",
    "\n",
    "\n",
    "print([a.shape for a in [W_cv, W_vm, W_vd, W_vp]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, vstack\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def parallel_multiply_chunk(W1_csr, W2_csr, row_indices):\n",
    "    # Multiply a chunk of rows from W1_csr with W2_csr\n",
    "    result_chunk = W1_csr[row_indices].dot(W2_csr)\n",
    "    return result_chunk\n",
    "\n",
    "def M(W1, W2, n_jobs=-1):\n",
    "    # Convert to CSR format if not already\n",
    "    W1_csr = csr_matrix(W1) if not isinstance(W1, csr_matrix) else W1\n",
    "    W2_csr = csr_matrix(W2) if not isinstance(W2, csr_matrix) else W2\n",
    "\n",
    "    # Get the number of rows in W1\n",
    "    n_rows = W1_csr.shape[0]\n",
    "\n",
    "    # Determine the chunk size per job\n",
    "    chunk_size = n_rows // n_jobs if n_jobs > 1 else n_rows\n",
    "\n",
    "    # Create row indices to split the workload\n",
    "    row_chunks = [range(i, min(i + chunk_size, n_rows)) for i in range(0, n_rows, chunk_size)]\n",
    "\n",
    "    # Use Parallel to distribute the computation\n",
    "    print(f'multiplying {W1.shape} * {W2.shape} in parallel...')\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(parallel_multiply_chunk)(W1_csr, W2_csr, row_indices) for row_indices in row_chunks\n",
    "    )\n",
    "\n",
    "    # Stack the results vertically as a sparse matrix\n",
    "    result = vstack(results)\n",
    "    print(\"Done multiplication...\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "# Assuming W1 and W2 are large sparse matrices\n",
    "# W1 = ...\n",
    "# W2 = ...\n",
    "# Result = M(W1, W2, n_jobs=4)  # Example with 4 parallel jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PathCount\n",
    "\n",
    "PathCount here is as follow:\n",
    "for meta-path $p = CVMVC$, we say: $$PC_p = A_{CV} \\times A_{VM} \\times A_{VM}^T \\times A_{CV}^T $$\n",
    "This is equivalent to \n",
    "$$A_{CVM} = A_{CV} \\times A_{VM}$$\n",
    "$$PC_p = A_{CVM} \\times A_{CVM}^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "Patient-Patient completed!\n",
      "\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "visit-visit completed!\n",
      "\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "Diagnoses-Diagnoses completed!\n",
      "\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "Med-Med completed!\n",
      "\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (31869, 31869) * (31869, 31869) in parallel...\n",
      "Done multiplication...\n",
      "Proced-Proced completed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PathCount \n",
    "# Heterogeneous similarity\n",
    "\n",
    "W_CVM = M(W_cv, W_vm)\n",
    "W_CVD = M(W_cv, W_vd)\n",
    "W_CVP = M(W_cv, W_vp)\n",
    "\n",
    "W_DVM = M(W_vd.T, W_vm)\n",
    "W_DVP = M(W_vd.T, W_vp)\n",
    "W_PVM = M(W_vp.T, W_vm)\n",
    "\n",
    "# Homogeneous similarity\n",
    "# W_CVMVC = M(M(M(W_cv, W_vm), W_vm.T), W_cv.T)\n",
    "# W_CVDVC = M(M(M(W_cv, W_vd), W_vd.T), W_cv.T)\n",
    "# W_CVPVC = M(M(M(W_cv, W_vp), W_vp.T), W_cv.T)\n",
    "# W_DVMVD = M(M(M(W_vd.T, W_vm), W_vm.T), W_vd)\n",
    "# W_DVPVD = M(M(M(W_vd.T, W_vp), W_vp.T), W_vd)\n",
    "# W_MVDVM = M(M(M(W_vm.T, W_vd), W_vd.T), W_vm)\n",
    "# W_MVPVM = M(M(M(W_vm.T, W_vp), W_vp.T), W_vm)\n",
    "# W_PVDVP = M(M(M(W_vp.T, W_vd), W_vd.T), W_vp)\n",
    "# W_PVMVP = M(M(M(W_vp.T, W_vm), W_vm.T), W_vp)\n",
    "\n",
    "W_CVMVC = M(W_CVM, W_CVM.T)\n",
    "W_CVDVC = M(W_CVD, W_CVD.T)\n",
    "W_CVPVC = M(W_CVP, W_CVP.T)\n",
    "print('Patient-Patient completed!\\n')\n",
    "\n",
    "W_VMV = M(W_vm, W_vm.T)\n",
    "W_VDV = M(W_vd, W_vd.T)\n",
    "W_VPV = M(W_vp, W_vp.T)\n",
    "print('visit-visit completed!\\n')\n",
    "\n",
    "W_DVMVD = M(W_DVM, W_DVM.T)\n",
    "W_DVPVD = M(W_DVP, W_DVP.T)\n",
    "print('Diagnoses-Diagnoses completed!\\n')\n",
    "\n",
    "W_MVDVM = M(W_DVM.T, W_DVM)\n",
    "W_MVPVM = M(W_PVM.T, W_PVM)\n",
    "print('Med-Med completed!\\n')\n",
    "\n",
    "W_PVDVP = M(W_DVP.T, W_DVP)\n",
    "W_PVMVP = M(W_PVM, W_PVM.T)\n",
    "print('Proced-Proced completed!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric PathSim (SPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetricPathSim(G, p, Nodes):\n",
    "    '''SPS\n",
    "        G: heterogeneous graph,\n",
    "       p: meta-path, \n",
    "       |p| = 3,\n",
    "       return A(N by N).'''\n",
    "    n = len(Nodes)\n",
    "    A = np.zeros((n, n))\n",
    "    for i in range(0, n-1):\n",
    "        ni = Nodes[i]\n",
    "        if ni[0] == p[0]:\n",
    "            Ni = [x for x in G.neighbors(ni) if x[0]== p[1]]\n",
    "            for j in range(i+1, n):\n",
    "                nj = Nodes[j]\n",
    "                if nj[0] == p[-1]:\n",
    "                    Nj = [x for x in G.neighbors(nj) if x[0]== p[1]]\n",
    "                    # Now, we count len of common neighbors\n",
    "                    top_value = (len(set(Ni) & set(Nj)) + len(set(Nj) & set(Ni)))\n",
    "                    button_value = (len(set(Ni) & set(Ni)) + len(set(Nj) & set(Nj)))\n",
    "                    if button_value > 0 :\n",
    "                        A[i,j] =  top_value / button_value \n",
    "                        A[j,i] =  A[i,j]\n",
    "                    else:\n",
    "                        A[i,j] = 0\n",
    "                        A[j,i] = 0\n",
    "                    \n",
    "    return A\n",
    "\n",
    "def symmetricPathSim_2(PC, Nodes, selected_nodes):\n",
    "    '''SPS\n",
    "        G: heterogeneous graph,\n",
    "       p: meta-path, \n",
    "       |p| = 3,\n",
    "       return A(N by N).'''\n",
    "       \n",
    "    selected_indeces = [Nodes.index(n) for n in selected_nodes]\n",
    "    n = len(Nodes)\n",
    "    SPS = np.zeros((n, n))\n",
    "    for i in range(0, len(selected_nodes)-1):\n",
    "        ni = selected_indeces[i]\n",
    "        for j in range(i+1, len(selected_nodes)):\n",
    "            nj = selected_indeces[j]\n",
    "            SPS[ni, nj] = (PC[ni, nj] + PC[nj, ni]) / (PC[ni,ni] + PC[nj,nj])\n",
    "            SPS[nj, ni] = SPS[ni, nj]  # Ensure symmetry\n",
    "    return SPS\n",
    "\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "def calculate_sps(ni, nj):\n",
    "    # Check for zero diagonals to prevent division by zero\n",
    "    if PC_shared[ni, ni] + PC_shared[nj, nj] == 0:\n",
    "        return 0  # or handle in another way, e.g., return np.nan or a small constant\n",
    "\n",
    "    return (PC_shared[ni, nj] + PC_shared[nj, ni]) / (PC_shared[ni, ni] + PC_shared[nj, nj])\n",
    "\n",
    "def symmetricPathSim_3(PC, Nodes, selected_nodes):\n",
    "    '''SPS\n",
    "        G: heterogeneous graph,\n",
    "       p: meta-path, \n",
    "       |p| = 3,\n",
    "       return A(N by N).'''\n",
    "       \n",
    "    global PC_shared\n",
    "    PC_shared = PC  # Use shared memory\n",
    "\n",
    "    selected_indeces = [Nodes.index(n) for n in selected_nodes]\n",
    "    n = len(Nodes)\n",
    "    SPS = np.zeros((n, n))\n",
    "\n",
    "    # Prepare the pairs of indices for parallel processing\n",
    "    index_pairs = [(i, j) for i in range(len(selected_indeces) - 1)\n",
    "                          for j in range(i + 1, len(selected_indeces))]\n",
    "\n",
    "    # Use a backend that supports shared memory\n",
    "    with parallel_backend('loky', n_jobs=40):\n",
    "        results = Parallel()(delayed(calculate_sps)(selected_indeces[i], selected_indeces[j])\n",
    "                             for i, j in index_pairs)\n",
    "\n",
    "    # Populate the SPS matrix with the computed results\n",
    "    for idx, (i, j) in enumerate(index_pairs):\n",
    "        ni, nj = selected_indeces[i], selected_indeces[j]\n",
    "        SPS[ni, nj] = results[idx]\n",
    "        SPS[nj, ni] = results[idx]  # Ensure symmetry\n",
    "\n",
    "    return SPS\n",
    "\n",
    "\n",
    "\n",
    "# SPS_CVMVC = symmetricPathSim_3(W_CVMVC, Nodes2, P_train)\n",
    "# SPS_CVDVC = symmetricPathSim_3(W_CVDVC, Nodes2, P_train)\n",
    "# SPS_CVPVC = symmetricPathSim_3(W_CVPVC, Nodes2, P_train)\n",
    "# print('Patients-Patients completed!\\n')\n",
    "\n",
    "# SPS_VMV = symmetricPathSim_3(W_VMV, Nodes2, Visits)\n",
    "# SPS_VDV = symmetricPathSim_3(W_VDV, Nodes2, Visits)\n",
    "# SPS_VPV = symmetricPathSim_3(W_VPV, Nodes2, Visits)\n",
    "# print('visit-visit completed!\\n')\n",
    "\n",
    "# SPS_DVMVD = symmetricPathSim_3(W_DVMVD, Nodes2, Diagnosis)\n",
    "# SPS_DVPVD = symmetricPathSim_3(W_DVPVD, Nodes2, Diagnosis)\n",
    "# print('Diagnoses-Diagnoses completed!\\n')\n",
    "\n",
    "# SPS_MVDVM = symmetricPathSim_3(W_MVDVM, Nodes2, Medications)\n",
    "# SPS_MVPVM = symmetricPathSim_3(W_MVPVM, Nodes2, Medications)\n",
    "# print('Med-Med completed!\\n')\n",
    "\n",
    "# SPS_PVDVP = symmetricPathSim_3(W_PVDVP, Nodes2, Procedures)\n",
    "# SPS_PVMVP = symmetricPathSim_3(W_PVMVP, Nodes2, Procedures)\n",
    "# print('Proced-Proced completed!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of meta-paths = 22\n",
      "Matrix 0: 1677 non-zero elements\n",
      "\tSaving all non-zero values... (1677 non-zero elements)\n",
      "Matrix 1: 116239843 non-zero elements\n",
      "\tSaving one million non-zero values... (after reduction: 1023890 non-zero elements)\n",
      "Matrix 2: 86025154 non-zero elements\n",
      "\tSaving one million non-zero values... (after reduction: 1078408 non-zero elements)\n",
      "Matrix 3: 55665 non-zero elements\n",
      "\tSaving all non-zero values... (55665 non-zero elements)\n",
      "Matrix 4: 291137468 non-zero elements\n",
      "\tSaving one million non-zero values... (after reduction: 1037661 non-zero elements)\n",
      "Matrix 5: 173743531 non-zero elements\n",
      "\tSaving one million non-zero values... (after reduction: 1783166 non-zero elements)\n",
      "Matrix 6: 20445 non-zero elements\n",
      "\tSaving all non-zero values... (20445 non-zero elements)\n",
      "Matrix 7: 41209 non-zero elements\n",
      "\tSaving all non-zero values... (41209 non-zero elements)\n",
      "Matrix 8: 187264 non-zero elements\n",
      "\tSaving all non-zero values... (187264 non-zero elements)\n",
      "Matrix 9: 200765 non-zero elements\n",
      "\tSaving all non-zero values... (200765 non-zero elements)\n",
      "Matrix 10: 7736 non-zero elements\n",
      "\tSaving all non-zero values... (7736 non-zero elements)\n",
      "Matrix 11: 1225 non-zero elements\n",
      "\tSaving all non-zero values... (1225 non-zero elements)\n",
      "Matrix 12: 38634 non-zero elements\n",
      "\tSaving all non-zero values... (38634 non-zero elements)\n",
      "Matrix 13: 480012 non-zero elements\n",
      "\tSaving all non-zero values... (480012 non-zero elements)\n",
      "Matrix 14: 5062 non-zero elements\n",
      "\tSaving all non-zero values... (5062 non-zero elements)\n",
      "Matrix 15: 126894 non-zero elements\n",
      "\tSaving all non-zero values... (126894 non-zero elements)\n",
      "Matrix 16: 1935 non-zero elements\n",
      "\tSaving all non-zero values... (1935 non-zero elements)\n",
      "Matrix 17: 187825 non-zero elements\n",
      "\tSaving all non-zero values... (187825 non-zero elements)\n",
      "Matrix 18: 53103 non-zero elements\n",
      "\tSaving all non-zero values... (53103 non-zero elements)\n",
      "Matrix 19: 16199 non-zero elements\n",
      "\tSaving all non-zero values... (16199 non-zero elements)\n",
      "Matrix 20: 4614 non-zero elements\n",
      "\tSaving all non-zero values... (4614 non-zero elements)\n",
      "Matrix 21: 13141 non-zero elements\n",
      "\tSaving all non-zero values... (13141 non-zero elements)\n"
     ]
    }
   ],
   "source": [
    "def keep_top_million(sparse_matrix, top_n=1000000):\n",
    "    # Convert to CSR if the matrix is dense\n",
    "    if isinstance(sparse_matrix, np.ndarray):\n",
    "        sparse_matrix = sparse.csr_matrix(sparse_matrix)\n",
    "    \n",
    "    # Step 1: Find the threshold for the top million values\n",
    "    if sparse_matrix.nnz <= top_n:\n",
    "        # If the matrix has fewer non-zeros than top_n, return it as is\n",
    "        return sparse_matrix\n",
    "    \n",
    "    # Extract the non-zero data from the sparse matrix\n",
    "    sorted_data = np.sort(sparse_matrix.data)[-top_n]\n",
    "    \n",
    "    # Step 2: Filter the sparse matrix based on this threshold\n",
    "    mask = sparse_matrix.data >= sorted_data\n",
    "    \n",
    "    # Apply the mask to keep only values in the top million\n",
    "    filtered_data = sparse_matrix.data[mask]\n",
    "    filtered_indices = sparse_matrix.indices[mask]\n",
    "    filtered_indptr = np.zeros(sparse_matrix.shape[0] + 1, dtype=int)\n",
    "    \n",
    "    # Recalculate indptr array based on the filtered data\n",
    "    current_index = 0\n",
    "    for i in range(sparse_matrix.shape[0]):\n",
    "        row_start = sparse_matrix.indptr[i]\n",
    "        row_end = sparse_matrix.indptr[i+1]\n",
    "        \n",
    "        # Count non-zero elements in the current row that are in the top million\n",
    "        filtered_indptr[i+1] = filtered_indptr[i] + np.sum(mask[row_start:row_end])\n",
    "    \n",
    "    # Create a new sparse matrix with the filtered data\n",
    "    filtered_matrix = sparse.csr_matrix((filtered_data, filtered_indices, filtered_indptr), shape=sparse_matrix.shape)\n",
    "    \n",
    "    return filtered_matrix\n",
    "\n",
    "\n",
    "\n",
    "As_path = f'{saving_path}/A'\n",
    "if not os.path.exists(As_path):\n",
    "    os.makedirs(As_path)\n",
    "\n",
    "\n",
    "Ws = [W_CVMVC, W_CVDVC, W_CVPVC,          \n",
    "      W_VMV, W_VDV, W_VPV,          \n",
    "      W_DVMVD, W_DVPVD,         \n",
    "      W_MVPVM, W_MVDVM,        \n",
    "      W_PVDVP, W_PVMVP,        \n",
    "      W_cv, W_vd, W_vm, W_vp,       \n",
    "      W_CVM, W_CVD, W_CVP,      \n",
    "      W_DVM, W_PVM, W_DVP]\n",
    "\n",
    "# Ws = [SPS_CVMVC, SPS_CVDVC, SPS_CVPVC,          \n",
    "#       SPS_VMV, SPS_VDV, SPS_VPV,          \n",
    "#       SPS_DVMVD, SPS_DVPVD,         \n",
    "#       SPS_MVPVM, SPS_MVDVM,        \n",
    "#       SPS_PVDVP, SPS_PVMVP,\n",
    "#       W_cv, W_vd, W_vm, W_vp,       \n",
    "#       W_CVM, W_CVD, W_CVP,      \n",
    "#       W_DVM, W_PVM, W_DVP]\n",
    "      \n",
    "\n",
    "print(f'Number of meta-paths = {len(Ws)}')\n",
    "\n",
    "num_As = 0\n",
    "from scipy.sparse import issparse\n",
    "for i, A in enumerate(Ws):\n",
    "    # Check if A is sparse or dense\n",
    "    if issparse(A):\n",
    "        non_zero_count = A.count_nonzero()\n",
    "    else:\n",
    "        non_zero_count = np.count_nonzero(A)\n",
    "    \n",
    "    print(f\"Matrix {i}: {non_zero_count} non-zero elements\")\n",
    "    if non_zero_count > 0:\n",
    "        if non_zero_count > 1000000:\n",
    "            B = keep_top_million(A, top_n=1000000)\n",
    "            print(f\"\\tSaving one million non-zero values... (after reduction: {B.count_nonzero()} non-zero elements)\")\n",
    "        else:\n",
    "            B = A\n",
    "            print(f\"\\tSaving all non-zero values... ({non_zero_count} non-zero elements)\")\n",
    "\n",
    "        # Convert to CSR format if not already sparse\n",
    "        if not issparse(B):\n",
    "            B = sparse.csr_matrix(B)\n",
    "        \n",
    "        # Save the sparse matrix\n",
    "        sparse.save_npz(f\"{As_path}/sparse_matrix_{num_As}.npz\", B)\n",
    "        num_As+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert the As to edge-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done saving [unique edges]:  2909216\n",
      "Working on 0th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight0...\n",
      "Working on 1th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight1...\n",
      "Working on 2th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight2...\n",
      "Working on 3th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight3...\n",
      "Working on 4th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight4...\n",
      "Working on 5th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight5...\n",
      "Working on 6th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight6...\n",
      "Working on 7th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight7...\n",
      "Working on 8th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight8...\n",
      "Working on 9th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight9...\n",
      "Working on 10th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight10...\n",
      "Working on 11th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight11...\n",
      "Working on 12th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight12...\n",
      "Working on 13th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight13...\n",
      "Working on 14th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight14...\n",
      "Working on 15th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight15...\n",
      "Working on 16th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight16...\n",
      "Working on 17th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight17...\n",
      "Working on 18th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight18...\n",
      "Working on 19th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight19...\n",
      "Working on 20th file...\n",
      "\tdone...\n",
      "\tSaving...\n",
      "\tDone saving edge_weight20...\n"
     ]
    }
   ],
   "source": [
    "# num_As = len(Ws)\n",
    "# Check if the directory exists, and create it if it doesn't\n",
    "Nodes_path = f'{saving_path}/edges'\n",
    "if not os.path.exists(Nodes_path):\n",
    "    os.makedirs(Nodes_path)\n",
    "\n",
    "D = [get_edges_dict(f'{saving_path}/A/sparse_matrix_{i}.npz') for i in range(len(Ws)-1)]\n",
    "sorted_list_of_dicts = sorted(D, key=lambda x: len(x), reverse=True)\n",
    "\n",
    "unique_edges = set()\n",
    "\n",
    "for e in sorted_list_of_dicts:\n",
    "    unique_edges.update(e.keys())\n",
    "\n",
    "    \n",
    "with open(f'{saving_path}/edges/edge_list.pkl', 'wb') as file:\n",
    "    pickle.dump(unique_edges, file)\n",
    "\n",
    "print('done saving [unique edges]: ', len(unique_edges))\n",
    "\n",
    "for i, d in enumerate(D):\n",
    "    print(f'Working on {i}th file...')\n",
    "    results = []\n",
    "    for e in unique_edges:\n",
    "        if e in d:\n",
    "            results.append(d[e])\n",
    "        else:\n",
    "            results.append(0)\n",
    "    print(f'\\tdone...')\n",
    "    print('\\tSaving...')\n",
    "\n",
    "    with open(f'{saving_path}/edges/edge_weight{i}.pkl', 'wb') as file:\n",
    "        pickle.dump(results, file)\n",
    "\n",
    "    print(f'\\tDone saving edge_weight{i}...')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
